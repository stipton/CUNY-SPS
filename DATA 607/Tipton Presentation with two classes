---
title: "DATA 607 Presentation"
subtitle: "Confusion Matrices"
author: "Steve Tipton"
date: "March 14, 2018"
output:
  prettydoc::html_pretty:
    theme: Cayman
    highlight: github
---

## Previously....

A brave fellow student gave a presentation using the KNN matching algorithm to predict the class of an unknown (and possibly alien) species....

[*KNN Example* by Zach Dravis](http://rpubs.com/zdravis/368000)

Zach left us with a few questions at the end of the presentation, including:

* How to measure success [of the knn model]?
* How to empirically establish k?

In the comments section during the presentation, Prof. Catlin suggested using a confusion matrix to evaluate the models with different k values.  In this example, I will present the concept of the confusion matrix and apply it to Zach's KNN models.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
set.seed(1234)

HumanHeight <- rnorm(200, mean = 5.5, sd = .5)
WookieHeight <- rnorm(300, mean = 7.0, sd = .75)
##EwokHeight <- rnorm(300, mean = 4, sd = .5)

HumanWeight <- rnorm(200, mean = 150, sd = 30)
WookieWeight <- rnorm(300, mean = 200, sd = 50)
##EwokWeight <- rnorm(300, mean = 125, sd = 30)

## SWSpecies <- data.frame(Species = c(rep("Human", 200), rep("Wookie", 200), rep("Ewok", 200)), Height = c(HumanHeight, WookieHeight, EwokHeight), Weight = c(HumanWeight, WookieWeight, EwokWeight))
SWSpecies <- data.frame(Species = c(rep("Human", 300), rep("Wookie", 300)), Height = c(HumanHeight, WookieHeight), Weight = c(HumanWeight, WookieWeight))

library(ggplot2)
library(dplyr)

#Split into training and test
TrainingData <- SWSpecies %>%
  group_by(Species) %>%
  slice(1:133) %>%
  ungroup()

TestData <- SWSpecies %>%
  group_by(Species) %>%
  slice(134:200) %>%
  ungroup()

library(class)


library(knitr)
library(kableExtra)
```

## Ewoks and Humans and Wookies

For Zach's presentation, he created a data set containing 200 Ewoks, 200 humans, and 200 Wookies, then built a data frame with simulated height and weight data for each case.  Then, he used a training set of 399 observations from his data set to build two models with different $k$ values ($k=3$ and $k=9$), which were then applied to the remaining 201 observations (the test data set).

## Running the process with two different $k$ values

Code from Zach Dravis to apply the two models:

$k=3$

```{r}
SpeciesPrediction <- knn(train = select(TrainingData, Height, Weight), 
                         test = select(TestData, Height, Weight), 
                         cl = TrainingData$Species, 
                         k = 3)
FirstTest <- cbind(TestData, SpeciesPrediction)
FirstTest %>%  kable("html", caption = "Test1: k = 3") %>%
  kable_styling(bootstrap_options = c("striped")) %>%
  scroll_box(height = "500px")
```

$k=9$

```{r}
SpeciesPrediction <- knn(train = select(TrainingData, Height, Weight), 
                         test = select(TestData, Height, Weight), 
                         cl = TrainingData$Species, 
                         k = 9)
SecondTest <- cbind(TestData, SpeciesPrediction)
SecondTest %>%  kable("html", caption = "Test2: k = 9") %>%
  kable_styling(bootstrap_options = c("striped")) %>%
  scroll_box(height = "500px")
```

## Baseline: Random simulation

With the two given models from Zach's presentation, I wanted to create a baseline model as a control to test against.  I simulated data to assign the Species attribute randomly to a list of 67 Ewoks, 67 humans, and 67 Wookies.

From *Data Science for Business*: "Comparison against a random model establishes that there is some information to be extracted from the data."

If the results of our models are better than a random simulation, we're looking at the right features to use in our models.

```{r}
simSpeciesPrediction <- c(rep("Wookie", 100), rep("Ewok", 101))
randOrder <- sample(1:201, 201, replace = FALSE)
simSpeciesPrediction <- simSpeciesPrediction[randOrder]
BaseTest <- cbind(TestData, simSpeciesPrediction)
BaseTest %>%  kable("html", caption = "Baseline: Random") %>%
  kable_styling(bootstrap_options = c("striped")) %>%
  scroll_box(height = "500px")
```

## Is accuracy sufficient?

An intuitive first impulse to evaluate the model is to divide the number of correctly predicted results from the test data by the total number of observations in the test data.  This is known as Accuracy.

```{r}
#Accuracy of the Base model
(AccuracyBase <- sum(BaseTest$simSpeciesPrediction == BaseTest$Species)/length(BaseTest$simSpeciesPrediction))
#Accuracy of the FirstTest model (k = 3)
(AccuracyFirst <- sum(FirstTest$SpeciesPrediction == FirstTest$Species)/length(FirstTest$SpeciesPrediction))
#Accuracy of the SecondTest model (k = 9)
(AccuracySecond <- sum(SecondTest$SpeciesPrediction == SecondTest$Species)/length(SecondTest$SpeciesPrediction))
```

The test models have much greater accuracy than the random assignment simulation, so it seems like we're on the right track.  But the accuracy of a model doesn't tell the complete story of a model, and may in fact hide its flaws.

## Defining the Confusion Matrix

Instead of relying on the single metric of accuracy, we can create a confusion matrix to examine the different types of results the model generated.  To simplify this section, we will look at the set-up of a binary confusion matrix that predicts whether or not a given observation is a member of a class.

While accuracy gives us the proportion of correct results, the confusion matrix separates the correct results into two sets:

* True Positive (TP): the model correctly predicted YES 
* True Negative (TN): the model correctly predicted NO

The incorrect results are also divided in two:

* False Positive (FP): the model incorrectly predicted YES (actual NO) AKA Type I Error
* False Negative (FN): the model incorrectly predicted NO (actual YES) AKA Type II Error

![](ConfusionMatrix.png)

The correct predictions fall along the main diagonal of the matrix.

From the confusion matrix, many helpful statistics can be calculated to aid in analyzing the model.  The figure below gives a visual representation of the different statistics.

![https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values](wikiexample.png)

This figure also shows how simple accuracy can be misleading.  The accuracy = $\frac{20 + 1820}{2030} \approx 90.6\%$; however, looking at the confusion matrix gives better information - for example, the Positive predictive value is only 10%.

The [Data School](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) is a good resource for defining the statistics evaluated from the confusion matrix.

## Back to our Star Wars example

In R, the `confusionMatrix` function from the `caret` package returns the confusion matrix as well as several of the summary statistics we reviewed in the previous section.

We can create confusion matrices for our three species prediction models to evaluate their performance.  Since our data has three levels, the confusion matrix returns summary statistics for each feature separately.

Comparing the three models, we see that the two knn models fare significantly better than the baseline model based on random assignment.  

```{r, echo = FALSE, eval = FALSE, message = FALSE, warning = FALSE}
## manual calculation commented out
BaseConfusionMatrix <- data.frame(EwokActual = c(sum(BaseTest$Species == "Ewok" & BaseTest$simSpeciesPrediction == "Ewok"),
                                  sum(BaseTest$Species == "Ewok" & BaseTest$simSpeciesPrediction == "Human"),
                                  sum(BaseTest$Species == "Ewok" & BaseTest$simSpeciesPrediction == "Wookie")),
                              HumanActual = c(sum(BaseTest$Species == "Human" & BaseTest$simSpeciesPrediction == "Ewok"),
                                  sum(BaseTest$Species == "Human" & BaseTest$simSpeciesPrediction == "Human"),
                                  sum(BaseTest$Species == "Human" & BaseTest$simSpeciesPrediction == "Wookie")),
                              WookieActual = c(sum(BaseTest$Species == "Wookie" & BaseTest$simSpeciesPrediction == "Ewok"),
                                  sum(BaseTest$Species == "Wookie" & BaseTest$simSpeciesPrediction == "Human"),
                                  sum(BaseTest$Species == "Wookie" & BaseTest$simSpeciesPrediction == "Wookie")))
rownames(BaseConfusionMatrix) <- c("EwokResult", "HumanResult", "WookieResult")
BaseConfusionMatrix

FirstConfusionMatrix <- data.frame(EwokActual = c(sum(FirstTest$Species == "Ewok" & FirstTest$SpeciesPrediction == "Ewok"),
                                  sum(FirstTest$Species == "Ewok" & FirstTest$SpeciesPrediction == "Human"),
                                  sum(FirstTest$Species == "Ewok" & FirstTest$SpeciesPrediction == "Wookie")),
                              HumanActual = c(sum(FirstTest$Species == "Human" & FirstTest$SpeciesPrediction == "Ewok"),
                                  sum(FirstTest$Species == "Human" & FirstTest$SpeciesPrediction == "Human"),
                                  sum(FirstTest$Species == "Human" & FirstTest$SpeciesPrediction == "Wookie")),
                              WookieActual = c(sum(FirstTest$Species == "Wookie" & FirstTest$SpeciesPrediction == "Ewok"),
                                  sum(FirstTest$Species == "Wookie" & FirstTest$SpeciesPrediction == "Human"),
                                  sum(FirstTest$Species == "Wookie" & FirstTest$SpeciesPrediction == "Wookie")))
rownames(FirstConfusionMatrix) <- c("EwokResult", "HumanResult", "WookieResult")
FirstConfusionMatrix

SecondConfusionMatrix <- data.frame(EwokActual = c(sum(SecondTest$Species == "Ewok" & SecondTest$SpeciesPrediction == "Ewok"),
                                  sum(SecondTest$Species == "Ewok" & SecondTest$SpeciesPrediction == "Human"),
                                  sum(SecondTest$Species == "Ewok" & SecondTest$SpeciesPrediction == "Wookie")),
                              HumanActual = c(sum(SecondTest$Species == "Human" & SecondTest$SpeciesPrediction == "Ewok"),
                                  sum(SecondTest$Species == "Human" & SecondTest$SpeciesPrediction == "Human"),
                                  sum(SecondTest$Species == "Human" & SecondTest$SpeciesPrediction == "Wookie")),
                              WookieActual = c(sum(SecondTest$Species == "Wookie" & SecondTest$SpeciesPrediction == "Ewok"),
                                  sum(SecondTest$Species == "Wookie" & SecondTest$SpeciesPrediction == "Human"),
                                  sum(SecondTest$Species == "Wookie" & SecondTest$SpeciesPrediction == "Wookie")))
rownames(SecondConfusionMatrix) <- c("EwokResult", "HumanResult", "WookieResult")
SecondConfusionMatrix
```

```{r, message = FALSE}
library(caret)
confusionMatrix(BaseTest$Species, BaseTest$simSpeciesPrediction, dnn = c("Prediction", "Actual"))
```

```{r}
confusionMatrix(FirstTest$Species, FirstTest$SpeciesPrediction, dnn = c("Prediction", "Actual"))
```

```{r}
confusionMatrix(SecondTest$Species, SecondTest$SpeciesPrediction, dnn = c("Prediction", "Actual"))
```

It appears that the $k=3$ model performs better than the $k=9$ model. How do you choose the correct k?

## Values for $k$

From several sources online, a rule of thumb emerged to choose $k$ as the square root of the number of observations in the training set.  With our training set of 399 observations, how would the knn model fare with $k=20$? 

[source 1](https://www3.nd.edu/~steve/computing_with_data/17_Refining_kNN/refining_knn.html)
[source 2](https://books.google.com/books?id=hIOBCgAAQBAJ&pg=PA580&lpg=PA580&dq=finding+best+k+for+knn+square+root&source=bl&ots=FySKAx26uX&sig=V3SF7Xww1-Icehzm91iWGOjopkY&hl=en&sa=X&ved=0ahUKEwjYh9GFr_DMAhXOsh4KHT3HCXkQ6AEIUzAI#v=onepage&q=finding%20best%20k%20for%20knn%20square%20root&f=false)

```{r}
## square root of 399 is approx. 20
k_pref <- TrainingData$Species %>%
  length() %>%
  sqrt() %>%
  round()

SpeciesPrediction <- knn(train = select(TrainingData, Height, Weight), 
                         test = select(TestData, Height, Weight), 
                         cl = TrainingData$Species, 
                         k = k_pref)
ThirdTest <- cbind(TestData, SpeciesPrediction)
ThirdTest %>%  kable("html", caption = "Test3: k = 20") %>%
  kable_styling(bootstrap_options = c("striped")) %>%
  scroll_box(height = "500px")

(AccuracyThird <- sum(ThirdTest$SpeciesPrediction == ThirdTest$Species)/length(ThirdTest$SpeciesPrediction))

confusionMatrix(ThirdTest$Species, ThirdTest$SpeciesPrediction, dnn = c("Prediction", "Actual"))
```

(https://cran.r-project.org/web/packages/heuristica/heuristica.pdf)

```{r}
library(heuristica)
AccuracyVector <- numeric() 
Stats_df <- data.frame(Accuracy = double(),
                 Sensitivy = double(),
                 Specificity = double(),
                 Precision = double(),
                 stringsAsFactors=FALSE)
for(i in 1:30) {
  SpeciesPrediction <- knn(train = select(TrainingData, Height, Weight), 
                         test = select(TestData, Height, Weight), 
                         cl = TrainingData$Species, 
                         k = i)
  Test <- cbind(TestData, SpeciesPrediction)
  AccuracyVector[i] <- sum(Test$SpeciesPrediction == Test$Species)/length(Test$SpeciesPrediction)
  CM <- confusionMatrix(Test$Species, Test$simSpeciesPrediction, dnn = c("Prediction", "Actual"))
  Stats_df[,i] <- unlist(statsFromConfusionMatrix(CM))
}
Stats_df
Acc <- cbind.data.frame(k = 1:30, AccuracyVector)
ggplot(Acc, aes(k, AccuracyVector)) + geom_line()
##Acc <- cbind.data.frame(k = 1:30, AccuracyVector)
```
